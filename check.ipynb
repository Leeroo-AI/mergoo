{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = peft.AutoPeftModel.from_pretrained(\"nlee-208/phi-sft-capy-full-lora\")\n",
    "# # model = peft.PeftModel.from_pretrained(model\"nlee-208/phi-sft-capy-full-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00,  4.67it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# config = PeftConfig.from_pretrained(\"nlee-208/phi-sft-capy-full-lora\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "# # lora_model = PeftModel.from_pretrained(model, \"nlee-208/phi-sft-capy-full-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors: 100%|██████████| 42.0M/42.0M [00:00<00:00, 107MB/s] \n"
     ]
    }
   ],
   "source": [
    "# lora_model = PeftModel.from_pretrained(model, \"nlee-208/phi-sft-capy-full-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE Layer Index : [*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router_layers : ['fc2', 'fc1']\n",
      "adapter_config.target_modules : {'fc2', 'fc1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 709/709 [00:00<00:00, 394921.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_averaged_layers : 325\n",
      "count_router_layers : 384\n",
      "count_total_router_layers : 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import peft \n",
    "import transformers\n",
    "import os\n",
    "import torch\n",
    "from mergoo.compose_experts import ComposeExperts\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"HF_TOKEN\"]='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'\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "\n",
    "config = {\n",
    "    \"model_type\": \"phi\",\n",
    "    \"num_experts_per_tok\": 2,\n",
    "    \"base_model\": \"microsoft/phi-2\",\n",
    "    \"experts\": [\n",
    "        {\"expert_name\": \"adapter_1\", \"model_id\": \"nlee-208/phi-sft-capy_s1-ff-lora\"},\n",
    "        {\"expert_name\": \"adapter_2\", \"model_id\": \"nlee-208/phi-sft-capy_s2-ff-lora\"},\n",
    "        {\"expert_name\": \"adapter_3\", \"model_id\": \"nlee-208/phi-sft-capy_s3-ff-lora\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "# create checkpoint\n",
    "model_id = 'mop-phi-sft-capy_s123-ff-lora'\n",
    "expertmerger = ComposeExperts(config, torch_dtype=torch.bfloat16)\n",
    "expertmerger.compose()\n",
    "# expertmerger.save_checkpoint(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'phi',\n",
       " 'num_experts_per_tok': 1,\n",
       " 'base_model': 'microsoft/phi-2',\n",
       " 'experts': [{'expert_name': 'adapter_1',\n",
       "   'model_id': 'nlee-208/phi-sft-capy_s1-ff-lora'},\n",
       "  {'expert_name': 'adapter_2',\n",
       "   'model_id': 'nlee-208/phi-sft-capy_s2-ff-lora'}],\n",
       " 'router_layers_index': [],\n",
       " 'adapter_configs': [{'peft_type': <PeftType.LORA: 'LORA'>,\n",
       "   'auto_mapping': None,\n",
       "   'base_model_name_or_path': 'microsoft/phi-2',\n",
       "   'revision': None,\n",
       "   'task_type': 'CAUSAL_LM',\n",
       "   'inference_mode': True,\n",
       "   'r': 16,\n",
       "   'target_modules': \"{'fc2', 'fc1'}\",\n",
       "   'lora_alpha': 16,\n",
       "   'lora_dropout': 0.05,\n",
       "   'fan_in_fan_out': False,\n",
       "   'bias': 'none',\n",
       "   'use_rslora': False,\n",
       "   'modules_to_save': None,\n",
       "   'init_lora_weights': True,\n",
       "   'layers_to_transform': None,\n",
       "   'layers_pattern': None,\n",
       "   'rank_pattern': {},\n",
       "   'alpha_pattern': {},\n",
       "   'megatron_config': None,\n",
       "   'megatron_core': 'megatron.core',\n",
       "   'loftq_config': {},\n",
       "   'use_dora': False,\n",
       "   'layer_replication': None},\n",
       "  {'peft_type': <PeftType.LORA: 'LORA'>,\n",
       "   'auto_mapping': None,\n",
       "   'base_model_name_or_path': 'microsoft/phi-2',\n",
       "   'revision': None,\n",
       "   'task_type': 'CAUSAL_LM',\n",
       "   'inference_mode': True,\n",
       "   'r': 16,\n",
       "   'target_modules': \"{'fc2', 'fc1'}\",\n",
       "   'lora_alpha': 16,\n",
       "   'lora_dropout': 0.05,\n",
       "   'fan_in_fan_out': False,\n",
       "   'bias': 'none',\n",
       "   'use_rslora': False,\n",
       "   'modules_to_save': None,\n",
       "   'init_lora_weights': True,\n",
       "   'layers_to_transform': None,\n",
       "   'layers_pattern': None,\n",
       "   'rank_pattern': {},\n",
       "   'alpha_pattern': {},\n",
       "   'megatron_config': None,\n",
       "   'megatron_core': 'megatron.core',\n",
       "   'loftq_config': {},\n",
       "   'use_dora': False,\n",
       "   'layer_replication': None}],\n",
       " 'router_layers': ['fc2', 'fc1']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expertmerger.composer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved in mop-phi-sft-capy_s123-ff-lora/model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved at mop-phi-sft-capy_s123-ff-lora\n"
     ]
    }
   ],
   "source": [
    "expertmerger.save_checkpoint(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userhomes/noah/miniconda3/envs/lang/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of MOPPhiForCausalLM were not initialized from the model checkpoint at mop-phi-sft-capy_s123-ff-lora and are newly initialized: ['model.layers.0.mlp.fc1.gate.weight', 'model.layers.0.mlp.fc2.gate.weight', 'model.layers.1.mlp.fc1.gate.weight', 'model.layers.1.mlp.fc2.gate.weight', 'model.layers.10.mlp.fc1.gate.weight', 'model.layers.10.mlp.fc2.gate.weight', 'model.layers.11.mlp.fc1.gate.weight', 'model.layers.11.mlp.fc2.gate.weight', 'model.layers.12.mlp.fc1.gate.weight', 'model.layers.12.mlp.fc2.gate.weight', 'model.layers.13.mlp.fc1.gate.weight', 'model.layers.13.mlp.fc2.gate.weight', 'model.layers.14.mlp.fc1.gate.weight', 'model.layers.14.mlp.fc2.gate.weight', 'model.layers.15.mlp.fc1.gate.weight', 'model.layers.15.mlp.fc2.gate.weight', 'model.layers.16.mlp.fc1.gate.weight', 'model.layers.16.mlp.fc2.gate.weight', 'model.layers.17.mlp.fc1.gate.weight', 'model.layers.17.mlp.fc2.gate.weight', 'model.layers.18.mlp.fc1.gate.weight', 'model.layers.18.mlp.fc2.gate.weight', 'model.layers.19.mlp.fc1.gate.weight', 'model.layers.19.mlp.fc2.gate.weight', 'model.layers.2.mlp.fc1.gate.weight', 'model.layers.2.mlp.fc2.gate.weight', 'model.layers.20.mlp.fc1.gate.weight', 'model.layers.20.mlp.fc2.gate.weight', 'model.layers.21.mlp.fc1.gate.weight', 'model.layers.21.mlp.fc2.gate.weight', 'model.layers.22.mlp.fc1.gate.weight', 'model.layers.22.mlp.fc2.gate.weight', 'model.layers.23.mlp.fc1.gate.weight', 'model.layers.23.mlp.fc2.gate.weight', 'model.layers.24.mlp.fc1.gate.weight', 'model.layers.24.mlp.fc2.gate.weight', 'model.layers.25.mlp.fc1.gate.weight', 'model.layers.25.mlp.fc2.gate.weight', 'model.layers.26.mlp.fc1.gate.weight', 'model.layers.26.mlp.fc2.gate.weight', 'model.layers.27.mlp.fc1.gate.weight', 'model.layers.27.mlp.fc2.gate.weight', 'model.layers.28.mlp.fc1.gate.weight', 'model.layers.28.mlp.fc2.gate.weight', 'model.layers.29.mlp.fc1.gate.weight', 'model.layers.29.mlp.fc2.gate.weight', 'model.layers.3.mlp.fc1.gate.weight', 'model.layers.3.mlp.fc2.gate.weight', 'model.layers.30.mlp.fc1.gate.weight', 'model.layers.30.mlp.fc2.gate.weight', 'model.layers.31.mlp.fc1.gate.weight', 'model.layers.31.mlp.fc2.gate.weight', 'model.layers.4.mlp.fc1.gate.weight', 'model.layers.4.mlp.fc2.gate.weight', 'model.layers.5.mlp.fc1.gate.weight', 'model.layers.5.mlp.fc2.gate.weight', 'model.layers.6.mlp.fc1.gate.weight', 'model.layers.6.mlp.fc2.gate.weight', 'model.layers.7.mlp.fc1.gate.weight', 'model.layers.7.mlp.fc2.gate.weight', 'model.layers.8.mlp.fc1.gate.weight', 'model.layers.8.mlp.fc2.gate.weight', 'model.layers.9.mlp.fc1.gate.weight', 'model.layers.9.mlp.fc2.gate.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mergoo.models.modeling_phi import MOPPhiForCausalLM\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "model = MOPPhiForCausalLM.from_pretrained(\n",
    "    \"mop-phi-sft-capy_s123-ff-lora\",\n",
    "    # device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the model weights which are not gates\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    if \"gate\" not in name:\n",
    "        params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.fc1.gate.weight 5120\n",
      "model.layers.0.mlp.fc2.gate.weight 5120\n",
      "model.layers.1.mlp.fc1.gate.weight 5120\n",
      "model.layers.1.mlp.fc2.gate.weight 5120\n",
      "model.layers.2.mlp.fc1.gate.weight 5120\n",
      "model.layers.2.mlp.fc2.gate.weight 5120\n",
      "model.layers.3.mlp.fc1.gate.weight 5120\n",
      "model.layers.3.mlp.fc2.gate.weight 5120\n",
      "model.layers.4.mlp.fc1.gate.weight 5120\n",
      "model.layers.4.mlp.fc2.gate.weight 5120\n",
      "model.layers.5.mlp.fc1.gate.weight 5120\n",
      "model.layers.5.mlp.fc2.gate.weight 5120\n",
      "model.layers.6.mlp.fc1.gate.weight 5120\n",
      "model.layers.6.mlp.fc2.gate.weight 5120\n",
      "model.layers.7.mlp.fc1.gate.weight 5120\n",
      "model.layers.7.mlp.fc2.gate.weight 5120\n",
      "model.layers.8.mlp.fc1.gate.weight 5120\n",
      "model.layers.8.mlp.fc2.gate.weight 5120\n",
      "model.layers.9.mlp.fc1.gate.weight 5120\n",
      "model.layers.9.mlp.fc2.gate.weight 5120\n",
      "model.layers.10.mlp.fc1.gate.weight 5120\n",
      "model.layers.10.mlp.fc2.gate.weight 5120\n",
      "model.layers.11.mlp.fc1.gate.weight 5120\n",
      "model.layers.11.mlp.fc2.gate.weight 5120\n",
      "model.layers.12.mlp.fc1.gate.weight 5120\n",
      "model.layers.12.mlp.fc2.gate.weight 5120\n",
      "model.layers.13.mlp.fc1.gate.weight 5120\n",
      "model.layers.13.mlp.fc2.gate.weight 5120\n",
      "model.layers.14.mlp.fc1.gate.weight 5120\n",
      "model.layers.14.mlp.fc2.gate.weight 5120\n",
      "model.layers.15.mlp.fc1.gate.weight 5120\n",
      "model.layers.15.mlp.fc2.gate.weight 5120\n",
      "model.layers.16.mlp.fc1.gate.weight 5120\n",
      "model.layers.16.mlp.fc2.gate.weight 5120\n",
      "model.layers.17.mlp.fc1.gate.weight 5120\n",
      "model.layers.17.mlp.fc2.gate.weight 5120\n",
      "model.layers.18.mlp.fc1.gate.weight 5120\n",
      "model.layers.18.mlp.fc2.gate.weight 5120\n",
      "model.layers.19.mlp.fc1.gate.weight 5120\n",
      "model.layers.19.mlp.fc2.gate.weight 5120\n",
      "model.layers.20.mlp.fc1.gate.weight 5120\n",
      "model.layers.20.mlp.fc2.gate.weight 5120\n",
      "model.layers.21.mlp.fc1.gate.weight 5120\n",
      "model.layers.21.mlp.fc2.gate.weight 5120\n",
      "model.layers.22.mlp.fc1.gate.weight 5120\n",
      "model.layers.22.mlp.fc2.gate.weight 5120\n",
      "model.layers.23.mlp.fc1.gate.weight 5120\n",
      "model.layers.23.mlp.fc2.gate.weight 5120\n",
      "model.layers.24.mlp.fc1.gate.weight 5120\n",
      "model.layers.24.mlp.fc2.gate.weight 5120\n",
      "model.layers.25.mlp.fc1.gate.weight 5120\n",
      "model.layers.25.mlp.fc2.gate.weight 5120\n",
      "model.layers.26.mlp.fc1.gate.weight 5120\n",
      "model.layers.26.mlp.fc2.gate.weight 5120\n",
      "model.layers.27.mlp.fc1.gate.weight 5120\n",
      "model.layers.27.mlp.fc2.gate.weight 5120\n",
      "model.layers.28.mlp.fc1.gate.weight 5120\n",
      "model.layers.28.mlp.fc2.gate.weight 5120\n",
      "model.layers.29.mlp.fc1.gate.weight 5120\n",
      "model.layers.29.mlp.fc2.gate.weight 5120\n",
      "model.layers.30.mlp.fc1.gate.weight 5120\n",
      "model.layers.30.mlp.fc2.gate.weight 5120\n",
      "model.layers.31.mlp.fc1.gate.weight 5120\n",
      "model.layers.31.mlp.fc2.gate.weight 5120\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for name, params in model.named_parameters():\n",
    "    # print(name)\n",
    "    if \"gate\" in name:\n",
    "        # a += params.numel()\n",
    "        print(name, params.numel())\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327680"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5120 * 32 *2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_type\": \"mistral\",\n",
    "    \"num_experts_per_tok\": 2,\n",
    "    \"base_model\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"experts\": [\n",
    "        {\"expert_name\": \"adapter_1\", \"model_id\": \"predibase/customer_support\"},\n",
    "        {\"expert_name\": \"adapter_2\", \"model_id\": \"predibase/customer_support_accounts\"},\n",
    "        {\"expert_name\": \"adapter_3\", \"model_id\": \"predibase/customer_support_orders\"},\n",
    "        {\"expert_name\": \"adapter_4\", \"model_id\": \"predibase/customer_support_payments\"}\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE Layer Index : [*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.40s/it]\n",
      "generation_config.json: 100%|██████████| 116/116 [00:00<00:00, 15.5kB/s]\n",
      "adapter_config.json: 100%|██████████| 600/600 [00:00<00:00, 315kB/s]\n",
      "adapter_model.safetensors: 100%|██████████| 13.6M/13.6M [00:00<00:00, 32.6MB/s]\n",
      "100%|██████████| 803/803 [00:00<00:00, 344097.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_averaged_layers : 227\n",
      "count_router_layers : 576\n",
      "count_total_router_layers : 576\n",
      "The model is bigger than the maximum size per checkpoint (9GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at data/mistral_lora_moe/model.safetensors.index.json.\n",
      "checkpoint saved at data/mistral_lora_moe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mergoo.compose_experts import ComposeExperts\n",
    "\n",
    "# create checkpoint\n",
    "model_id = \"data/mistral_lora_moe\"\n",
    "expertmerger = ComposeExperts(config, torch_dtype=torch.float16)\n",
    "expertmerger.compose()\n",
    "expertmerger.save_checkpoint(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]\n",
      "Some weights of MistralForCausalLM were not initialized from the model checkpoint at data/mistral_lora_moe and are newly initialized: ['model.layers.0.self_attn.q_proj.gate.weight', 'model.layers.0.self_attn.v_proj.gate.weight', 'model.layers.1.self_attn.q_proj.gate.weight', 'model.layers.1.self_attn.v_proj.gate.weight', 'model.layers.10.self_attn.q_proj.gate.weight', 'model.layers.10.self_attn.v_proj.gate.weight', 'model.layers.11.self_attn.q_proj.gate.weight', 'model.layers.11.self_attn.v_proj.gate.weight', 'model.layers.12.self_attn.q_proj.gate.weight', 'model.layers.12.self_attn.v_proj.gate.weight', 'model.layers.13.self_attn.q_proj.gate.weight', 'model.layers.13.self_attn.v_proj.gate.weight', 'model.layers.14.self_attn.q_proj.gate.weight', 'model.layers.14.self_attn.v_proj.gate.weight', 'model.layers.15.self_attn.q_proj.gate.weight', 'model.layers.15.self_attn.v_proj.gate.weight', 'model.layers.16.self_attn.q_proj.gate.weight', 'model.layers.16.self_attn.v_proj.gate.weight', 'model.layers.17.self_attn.q_proj.gate.weight', 'model.layers.17.self_attn.v_proj.gate.weight', 'model.layers.18.self_attn.q_proj.gate.weight', 'model.layers.18.self_attn.v_proj.gate.weight', 'model.layers.19.self_attn.q_proj.gate.weight', 'model.layers.19.self_attn.v_proj.gate.weight', 'model.layers.2.self_attn.q_proj.gate.weight', 'model.layers.2.self_attn.v_proj.gate.weight', 'model.layers.20.self_attn.q_proj.gate.weight', 'model.layers.20.self_attn.v_proj.gate.weight', 'model.layers.21.self_attn.q_proj.gate.weight', 'model.layers.21.self_attn.v_proj.gate.weight', 'model.layers.22.self_attn.q_proj.gate.weight', 'model.layers.22.self_attn.v_proj.gate.weight', 'model.layers.23.self_attn.q_proj.gate.weight', 'model.layers.23.self_attn.v_proj.gate.weight', 'model.layers.24.self_attn.q_proj.gate.weight', 'model.layers.24.self_attn.v_proj.gate.weight', 'model.layers.25.self_attn.q_proj.gate.weight', 'model.layers.25.self_attn.v_proj.gate.weight', 'model.layers.26.self_attn.q_proj.gate.weight', 'model.layers.26.self_attn.v_proj.gate.weight', 'model.layers.27.self_attn.q_proj.gate.weight', 'model.layers.27.self_attn.v_proj.gate.weight', 'model.layers.28.self_attn.q_proj.gate.weight', 'model.layers.28.self_attn.v_proj.gate.weight', 'model.layers.29.self_attn.q_proj.gate.weight', 'model.layers.29.self_attn.v_proj.gate.weight', 'model.layers.3.self_attn.q_proj.gate.weight', 'model.layers.3.self_attn.v_proj.gate.weight', 'model.layers.30.self_attn.q_proj.gate.weight', 'model.layers.30.self_attn.v_proj.gate.weight', 'model.layers.31.self_attn.q_proj.gate.weight', 'model.layers.31.self_attn.v_proj.gate.weight', 'model.layers.4.self_attn.q_proj.gate.weight', 'model.layers.4.self_attn.v_proj.gate.weight', 'model.layers.5.self_attn.q_proj.gate.weight', 'model.layers.5.self_attn.v_proj.gate.weight', 'model.layers.6.self_attn.q_proj.gate.weight', 'model.layers.6.self_attn.v_proj.gate.weight', 'model.layers.7.self_attn.q_proj.gate.weight', 'model.layers.7.self_attn.v_proj.gate.weight', 'model.layers.8.self_attn.q_proj.gate.weight', 'model.layers.8.self_attn.v_proj.gate.weight', 'model.layers.9.self_attn.q_proj.gate.weight', 'model.layers.9.self_attn.v_proj.gate.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from mergoo.models.modeling_mistral import MistralForCausalLM\n",
    "\n",
    "model = MistralForCausalLM.from_pretrained(\"data/mistral_lora_moe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.0.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.0.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.0.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.0.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.0.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.0.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.0.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "model.layers.0.self_attn.q_proj.gate.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.0.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.0.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.0.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.0.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.0.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.0.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.0.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "model.layers.0.self_attn.v_proj.gate.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.1.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.1.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.1.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.1.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.1.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.1.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.1.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "model.layers.1.self_attn.q_proj.gate.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.1.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.1.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.1.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.1.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.1.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.1.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.1.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "model.layers.1.self_attn.v_proj.gate.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.2.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.2.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.2.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.2.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.2.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.2.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.2.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "model.layers.2.self_attn.q_proj.gate.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.2.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.2.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.2.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.2.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.2.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.2.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.2.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "model.layers.2.self_attn.v_proj.gate.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.3.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.3.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.3.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.3.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.3.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.3.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.3.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "model.layers.3.self_attn.q_proj.gate.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.3.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.3.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.3.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.3.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.3.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.3.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.3.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "model.layers.3.self_attn.v_proj.gate.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.4.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.4.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.4.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.4.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.4.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.4.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.4.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "model.layers.4.self_attn.q_proj.gate.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.4.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.4.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.4.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.4.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.4.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.4.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.4.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "model.layers.4.self_attn.v_proj.gate.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.5.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.5.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.5.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.5.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.5.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.5.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.5.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "model.layers.5.self_attn.q_proj.gate.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.5.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.5.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.5.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.5.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.5.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.5.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.5.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "model.layers.5.self_attn.v_proj.gate.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.6.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.6.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.6.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.6.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.6.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.6.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.6.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "model.layers.6.self_attn.q_proj.gate.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.6.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.6.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.6.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.6.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.6.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.6.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.6.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "model.layers.6.self_attn.v_proj.gate.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.7.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.7.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.7.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.7.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.7.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.7.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.7.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "model.layers.7.self_attn.q_proj.gate.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.7.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.7.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.7.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.7.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.7.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.7.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.7.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "model.layers.7.self_attn.v_proj.gate.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.8.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.8.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.8.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.8.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.8.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.8.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.8.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "model.layers.8.self_attn.q_proj.gate.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.8.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.8.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.8.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.8.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.8.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.8.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.8.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "model.layers.8.self_attn.v_proj.gate.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.9.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.9.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.9.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.9.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.9.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.9.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.9.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "model.layers.9.self_attn.q_proj.gate.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.9.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.9.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.9.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.9.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.9.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.9.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.9.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "model.layers.9.self_attn.v_proj.gate.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.10.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.10.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.10.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.10.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.10.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.10.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.10.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "model.layers.10.self_attn.q_proj.gate.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.10.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.10.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.10.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.10.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.10.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.10.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.10.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "model.layers.10.self_attn.v_proj.gate.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.11.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.11.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.11.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.11.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.11.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.11.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.11.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "model.layers.11.self_attn.q_proj.gate.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.11.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.11.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.11.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.11.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.11.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.11.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.11.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "model.layers.11.self_attn.v_proj.gate.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.12.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.12.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.12.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.12.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.12.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.12.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.12.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "model.layers.12.self_attn.q_proj.gate.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.12.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.12.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.12.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.12.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.12.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.12.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.12.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "model.layers.12.self_attn.v_proj.gate.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.13.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.13.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.13.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.13.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.13.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.13.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.13.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "model.layers.13.self_attn.q_proj.gate.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.13.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.13.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.13.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.13.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.13.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.13.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.13.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "model.layers.13.self_attn.v_proj.gate.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.14.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.14.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.14.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.14.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.14.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.14.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.14.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "model.layers.14.self_attn.q_proj.gate.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.14.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.14.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.14.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.14.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.14.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.14.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.14.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "model.layers.14.self_attn.v_proj.gate.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.15.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.15.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.15.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.15.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.15.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.15.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.15.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "model.layers.15.self_attn.q_proj.gate.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.15.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.15.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.15.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.15.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.15.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.15.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.15.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "model.layers.15.self_attn.v_proj.gate.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.16.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.16.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.16.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.16.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.16.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.16.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.16.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "model.layers.16.self_attn.q_proj.gate.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.16.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.16.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.16.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.16.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.16.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.16.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.16.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "model.layers.16.self_attn.v_proj.gate.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.17.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.17.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.17.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.17.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.17.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.17.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.17.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "model.layers.17.self_attn.q_proj.gate.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.17.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.17.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.17.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.17.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.17.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.17.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.17.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "model.layers.17.self_attn.v_proj.gate.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.18.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.18.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.18.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.18.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.18.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.18.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.18.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "model.layers.18.self_attn.q_proj.gate.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.18.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.18.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.18.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.18.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.18.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.18.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.18.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "model.layers.18.self_attn.v_proj.gate.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.19.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.19.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.19.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.19.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.19.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.19.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.19.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "model.layers.19.self_attn.q_proj.gate.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.19.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.19.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.19.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.19.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.19.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.19.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.19.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "model.layers.19.self_attn.v_proj.gate.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.20.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.20.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.20.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.20.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.20.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.20.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.20.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "model.layers.20.self_attn.q_proj.gate.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.20.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.20.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.20.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.20.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.20.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.20.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.20.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "model.layers.20.self_attn.v_proj.gate.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.21.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.21.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.21.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.21.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.21.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.21.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.21.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "model.layers.21.self_attn.q_proj.gate.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.21.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.21.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.21.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.21.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.21.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.21.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.21.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "model.layers.21.self_attn.v_proj.gate.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.22.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.22.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.22.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.22.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.22.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.22.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.22.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "model.layers.22.self_attn.q_proj.gate.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.22.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.22.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.22.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.22.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.22.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.22.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.22.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "model.layers.22.self_attn.v_proj.gate.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.23.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.23.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.23.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.23.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.23.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.23.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.23.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "model.layers.23.self_attn.q_proj.gate.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.23.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.23.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.23.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.23.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.23.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.23.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.23.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "model.layers.23.self_attn.v_proj.gate.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.24.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.24.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.24.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.24.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.24.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.24.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.24.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "model.layers.24.self_attn.q_proj.gate.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.24.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.24.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.24.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.24.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.24.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.24.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.24.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "model.layers.24.self_attn.v_proj.gate.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.25.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.25.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.25.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.25.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.25.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.25.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.25.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "model.layers.25.self_attn.q_proj.gate.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.25.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.25.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.25.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.25.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.25.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.25.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.25.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "model.layers.25.self_attn.v_proj.gate.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.26.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.26.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.26.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.26.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.26.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.26.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.26.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.26.self_attn.q_proj.base_layer.weight\n",
      "model.layers.26.self_attn.q_proj.gate.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.26.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.26.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.26.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.26.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.26.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.26.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.26.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.26.self_attn.v_proj.base_layer.weight\n",
      "model.layers.26.self_attn.v_proj.gate.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.27.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.27.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.27.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.27.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.27.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.27.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.27.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.27.self_attn.q_proj.base_layer.weight\n",
      "model.layers.27.self_attn.q_proj.gate.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.27.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.27.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.27.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.27.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.27.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.27.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.27.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.27.self_attn.v_proj.base_layer.weight\n",
      "model.layers.27.self_attn.v_proj.gate.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.28.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.28.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.28.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.28.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.28.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.28.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.28.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.28.self_attn.q_proj.base_layer.weight\n",
      "model.layers.28.self_attn.q_proj.gate.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.28.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.28.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.28.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.28.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.28.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.28.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.28.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.28.self_attn.v_proj.base_layer.weight\n",
      "model.layers.28.self_attn.v_proj.gate.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.29.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.29.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.29.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.29.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.29.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.29.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.29.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.29.self_attn.q_proj.base_layer.weight\n",
      "model.layers.29.self_attn.q_proj.gate.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.29.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.29.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.29.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.29.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.29.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.29.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.29.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.29.self_attn.v_proj.base_layer.weight\n",
      "model.layers.29.self_attn.v_proj.gate.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.30.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.30.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.30.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.30.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.30.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.30.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.30.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.30.self_attn.q_proj.base_layer.weight\n",
      "model.layers.30.self_attn.q_proj.gate.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.30.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.30.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.30.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.30.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.30.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.30.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.30.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.30.self_attn.v_proj.base_layer.weight\n",
      "model.layers.30.self_attn.v_proj.gate.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.lora_A.0.weight\n",
      "model.layers.31.self_attn.q_proj.lora_A.1.weight\n",
      "model.layers.31.self_attn.q_proj.lora_A.2.weight\n",
      "model.layers.31.self_attn.q_proj.lora_A.3.weight\n",
      "model.layers.31.self_attn.q_proj.lora_B.0.weight\n",
      "model.layers.31.self_attn.q_proj.lora_B.1.weight\n",
      "model.layers.31.self_attn.q_proj.lora_B.2.weight\n",
      "model.layers.31.self_attn.q_proj.lora_B.3.weight\n",
      "model.layers.31.self_attn.q_proj.base_layer.weight\n",
      "model.layers.31.self_attn.q_proj.gate.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.lora_A.0.weight\n",
      "model.layers.31.self_attn.v_proj.lora_A.1.weight\n",
      "model.layers.31.self_attn.v_proj.lora_A.2.weight\n",
      "model.layers.31.self_attn.v_proj.lora_A.3.weight\n",
      "model.layers.31.self_attn.v_proj.lora_B.0.weight\n",
      "model.layers.31.self_attn.v_proj.lora_B.1.weight\n",
      "model.layers.31.self_attn.v_proj.lora_B.2.weight\n",
      "model.layers.31.self_attn.v_proj.lora_B.3.weight\n",
      "model.layers.31.self_attn.v_proj.base_layer.weight\n",
      "model.layers.31.self_attn.v_proj.gate.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(name)\n",
    "    # if \"gate\" in name:\n",
    "    #     print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
